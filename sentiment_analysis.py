# -*- coding: utf-8 -*-
"""sentiment analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IMRQV1l2cwZ9ZhzsByHmfQmoh0aUf4Hf
"""



from google.colab import drive
drive.mount('/content/drive')

#Packages
!pip install contractions
!pip install pyngrok
import pandas as pd
import nltk
import requests
from bs4 import BeautifulSoup
import contractions
import re
from nltk.tokenize.toktok import ToktokTokenizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
from sklearn.pipeline import Pipeline
import joblib
from pyngrok import ngrok

df= pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')
df=df.iloc[:1000,:]
df.head()

nltk.download('stopwords')
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
len(stopword_list)

#HTML tag

def html_tag(text):
  soup = BeautifulSoup(text,"html.parser")
  new_text = soup.get_text()
  return new_text

#Expand Contractions

def con(text):
  expand = contractions.fix(text)
  return expand
con("Y'all can't expand I'd think")

#Removing the Special Characters

def remove_sp(text):
  pattern = r'[^A-Za-z0-9\s]'
  text = re.sub(pattern,'',text)
  return text

#Remove Stopwords

tokenizer = ToktokTokenizer()
def remove_stopwords(text):
  tokens = tokenizer.tokenize(text)
  tokens = [token.strip() for token in tokens]
  filtered_tokens = [token for token in tokens if token not in stopword_list]
  filtered_text = ' '.join(filtered_tokens)
  return filtered_text

df.review = df.review.apply(lambda x:x.lower())
df.review = df.review.apply(html_tag)
df.review = df.review.apply(con)
df.review = df.review.apply(remove_sp)
df.review = df.review.apply(remove_stopwords)

df.head()

x=df['review'].values
y=df['sentiment'].values

x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)
print(x_train.shape)
print(x_test.shape)

vect = TfidfVectorizer()
x_train_v = vect.fit_transform(x_train)
x_test_v = vect.transform(x_test)

model = SVC()
model.fit(x_train_v,y_train)
y_pred = model.predict(x_test_v)
y_pred

accuracy_score(y_pred,y_test)

confusion_matrix(y_pred,y_test)

print(classification_report(y_pred,y_test))

# Evaluating for a specific message

text = df['review'][10]
print(text)
df['sentiment'][10]

text=vect.transform([text])
model.predict(text)

text_model = Pipeline([('vect',TfidfVectorizer()),('model',SVC())])

text_model.fit(x_train,y_train)

y_pred=text_model.predict(x_test)

# Evaluating for a specific message

text = df['review'][10]
text_model.predict([text])

joblib.dump(text_model,'sentiment_analyzer')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# model=joblib.load('sentiment_analyzer')
# st.title('Sentiment Analyzer')
# ip = st.text_input("Enter the Review")
# op = model.predict([ip])
# if st.button('Predict'):
#   st.title(op[0])

!pip install streamlit
!pip install pyngrok

import subprocess

# Run the curl command using subprocess
subprocess.run(["curl", "http://localhost:8501"])

!pkill ngrok

from pyngrok import ngrok

# Set the authtoken (replace with your actual token)
ngrok.set_auth_token("2aFaJAZdPj3tN0moeQtkNZwzUpm_3FvjxDCJdajDAMfrvkeUR")

# Start Streamlit
import os
os.system("nohup streamlit run app.py &")

# Wait a few seconds, then connect Ngrok to port 8501
!sleep 5
url = ngrok.connect(8501)  # Notice the port should be passed as an integer, not a string
print(f"Access your Streamlit app at: {url}")

